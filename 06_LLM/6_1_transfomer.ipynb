{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9616616,
          "sourceType": "datasetVersion",
          "datasetId": 5868794
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "EJh-4Y7tlzgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "7AXz1UcilzgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Accuracy Metric\n",
        "\n",
        "    Args:\n",
        "        y_pred (np.array): model prediction of shape (batch_size,1).\n",
        "        y_true (np.array): ground truth of shape (batch_size,1).\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy value computed from given inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = y_pred == y_true\n",
        "    N = len(y_true)\n",
        "    acc = correct.sum() / N\n",
        "    return acc\n",
        "\n",
        "def get_cosine_decay_with_warmup(total_steps=1000, warmup_steps=100, max_lr=1e-3, min_lr=1e-7):\n",
        "\n",
        "    def get_lr(step):\n",
        "\n",
        "        if step < warmup_steps:\n",
        "            # Linear warmup\n",
        "            return max_lr * step / warmup_steps\n",
        "        else:\n",
        "            # Cosine decay\n",
        "            cosine_decay = 0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
        "            return min_lr + (max_lr - min_lr) * cosine_decay\n",
        "\n",
        "    return get_lr\n",
        "\n",
        "class LRScheduler:\n",
        "    def __init__(self, optimizer, lr_fn):\n",
        "        self.current_step = 0\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_fn = lr_fn\n",
        "\n",
        "    def step(self):\n",
        "        lr = self.lr_fn(self.current_step)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        self.current_step += 1\n",
        "        return lr\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets all the statistics.\"\"\"\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        \"\"\"Updates the meter with a new value.\n",
        "\n",
        "        Args:\n",
        "            val (float): The new value to update.\n",
        "            n (int): The number of occurrences of this value (default is 1).\n",
        "        \"\"\"\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "TcBn6k6PlzgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "# Decode tokens back to text\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Token IDs:\", tokens)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "id": "HP_sFGNvlzgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_dict = {'additional_special_tokens': ['<PAD>']}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "vocab = tokenizer.get_vocab()\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "bJ-wzXTjlzgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['<PAD>']"
      ],
      "metadata": {
        "id": "7zFLuYkPlzgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/wmt-sampled-50000-english-to-french-dataset/wmt_sample_50000.csv')\n",
        "df['en_encoded'] = tokenizer(df.en.tolist())['input_ids']\n",
        "df['fr_encoded'] = tokenizer(df.fr.tolist())['input_ids']\n",
        "df['en_len'] = df['en_encoded'].apply(len)\n",
        "df['fr_len'] = df['fr_encoded'].apply(len)\n",
        "df"
      ],
      "metadata": {
        "id": "P_qEVSNclzgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df['split'] = 'train'\n",
        "test_df['split'] = 'test'\n",
        "\n",
        "df = pd.concat([train_df, test_df]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "sAXpzHGQlzgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 클래스 정의\n",
        "class WMTDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.df.iloc[idx]\n",
        "        x = data['en_encoded']\n",
        "        y = data['fr_encoded']\n",
        "\n",
        "        x = torch.tensor(x)\n",
        "        y = torch.tensor(y)\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "zoTnqkBRlzgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, max_len=128, teacher_forcing=True, padding_side='right', pad_idx=vocab['<PAD>'], eos_idx=vocab['<|endoftext|>']):\n",
        "    #batch = [(x1,y1),(x2,y2)...]\n",
        "    x, y = zip(*batch) #(x1, x2, ...), (y1,y2,...)\n",
        "    x = [s[:max_len] for s in x]\n",
        "    y = [s[:max_len] for s in y]\n",
        "    x_lens = [len(s) for s in x]\n",
        "    y_lens = [len(s) for s in y]\n",
        "    max_lenx = max(x_lens)\n",
        "    max_leny = max(y_lens)\n",
        "\n",
        "    #add eos token\n",
        "    y = [torch.cat([s, torch.tensor([eos_idx])]) for s in y]\n",
        "\n",
        "    x = torch.stack([F.pad(s, (0,max_lenx-len(s)) if padding_side == 'right' else (max_lenx-len(s),0), value=pad_idx) for s in x])\n",
        "    y_tar = torch.stack([F.pad(s, (0,max_leny+1-len(s)) if padding_side == 'right' else (max_leny+1-len(s),0), value=pad_idx) for s in y])\n",
        "\n",
        "    if teacher_forcing:\n",
        "        #shifted-right input for teacher forcing\n",
        "        y_inp = torch.stack([F.pad(torch.roll(s,+1), (0,max_leny+1-len(s)) if padding_side == 'right' else (max_leny+1-len(s),0), value=pad_idx) for s in y])\n",
        "        return (x, y_inp), y_tar\n",
        "    else:\n",
        "        return x, y_tar"
      ],
      "metadata": {
        "id": "DXc5ds8blzgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = WMTDataset(df[df.split=='train'], tokenizer)\n",
        "test_dataset = WMTDataset(df[df.split=='test'], tokenizer)\n",
        "for x in train_dataset:\n",
        "    print(x)\n",
        "    break"
      ],
      "metadata": {
        "id": "Hu0cB39hlzgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "z7t1tPX_lzgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq + Attention Mechanism"
      ],
      "metadata": {
        "id": "Hl-jvjaMlzgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedRNN(nn.Module):\n",
        "    def __init__(self, rnn_module, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.rnn_module = rnn_module\n",
        "        assert rnn_module.batch_first\n",
        "\n",
        "    def forward(self, inputs, h=None, mask=None):\n",
        "        if mask is not None:\n",
        "            orig_len = inputs.size(1)\n",
        "            lens = mask.to(torch.int32).sum(1).to('cpu')\n",
        "            x = nn.utils.rnn.pack_padded_sequence(inputs, lengths=lens, batch_first=True, enforce_sorted=False)\n",
        "            x, hidden = self.rnn_module(x, h)\n",
        "            x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        else:\n",
        "            x, hidden = self.rnn_module(inputs, h)\n",
        "        return x, hidden\n",
        "\n",
        "class Seq2SeqAttentionModel(nn.Module):\n",
        "    def __init__(self, dim=256, vocab_size=len(vocab), pad_idx=vocab['<PAD>']):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim)\n",
        "        self.encoder_rnn = MaskedRNN(nn.GRU(dim, dim, num_layers=2, batch_first=True, bidirectional=True))\n",
        "        self.decoder_rnn = MaskedRNN(nn.GRU(dim, dim, num_layers=2, batch_first=True, bidirectional=False))\n",
        "        self.source_proj = nn.Linear(2*dim, dim)\n",
        "        self.target_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(2*dim, dim)\n",
        "        self.head = nn.Linear(dim, vocab_size)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x, y = inp\n",
        "        mask_x = x != self.pad_idx\n",
        "        mask_y = y != self.pad_idx\n",
        "        x = self.embedding(x)\n",
        "        y = self.embedding(y)\n",
        "        x, _ = self.encoder_rnn(x, mask=mask_x)\n",
        "        y, _ = self.decoder_rnn(y, mask=mask_y)\n",
        "        x = self.source_proj(x) #B,L_x,D\n",
        "        y = self.target_proj(y) #B,L_y,D\n",
        "\n",
        "        #dot-product attention\n",
        "        att = y @ x.transpose(-2,-1) #B,L_y,L_x\n",
        "        score = F.softmax(att, dim=-1)\n",
        "        c = score @ x #B,L_y,D\n",
        "\n",
        "        y = torch.cat([c,y], dim=-1)\n",
        "        y = self.out_proj(y)\n",
        "        y = self.head(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "n3u654YZlzgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_loader:\n",
        "    print(x[1].shape)\n",
        "    # print(x[0][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "PsKxa-eWlzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][1][0]"
      ],
      "metadata": {
        "id": "OYEd9hXflzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[1][0]"
      ],
      "metadata": {
        "id": "LyU9Lmuqlzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_loader:\n",
        "    break\n",
        "model = Seq2SeqAttentionModel()\n",
        "model(x[0]).shape"
      ],
      "metadata": {
        "id": "oPr2Pghalzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "ixnEtuwalzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedCCELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true, mask):\n",
        "        y_pred = y_pred[mask]\n",
        "        y_true = y_true[mask]\n",
        "        loss = F.cross_entropy(y_pred, y_true)\n",
        "        return loss.mean()\n",
        "\n",
        "def masked_accuracy(y_pred, y_true, mask):\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "    correct = y_pred == y_true\n",
        "    N = mask.to(correct.dtype).sum()\n",
        "    acc = correct.sum() / N\n",
        "    return acc"
      ],
      "metadata": {
        "id": "qeU3-GZClzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=42)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = Seq2SeqAttentionModel()\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 5\n",
        "clip_grad = 1.0\n",
        "lr = 1e-3\n",
        "\n",
        "loss_fn = MaskedCCELoss()#CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': [param for param in model.parameters() if param.ndim>=2], 'weight_decay': 0.01},\n",
        "    {'params': [param for param in model.parameters() if param.ndim<2], 'weight_decay': 0.0}\n",
        "], lr=lr)\n",
        "accum_loss = AverageMeter()\n",
        "accum_acc = AverageMeter()\n",
        "total_steps = len(train_loader) * epochs\n",
        "lr_fn = get_cosine_decay_with_warmup(total_steps=total_steps, warmup_steps=total_steps//10, max_lr=lr, min_lr=1e-7)\n",
        "scheduler = LRScheduler(optimizer, lr_fn)\n",
        "best_val_loss = float('inf') #initialize the best valiation loss as infinity\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()  # training mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    pbar = tqdm(train_loader, desc=f'TRAIN epoch {epoch}', total=len(train_loader))\n",
        "    for x, y in pbar:\n",
        "        x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "        mask = y != vocab['<PAD>']\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y, mask)\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        lr = scheduler.step()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        N = mask.int().sum()\n",
        "        accum_loss.update(loss.detach(), N)\n",
        "        accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{accum_loss.avg:.4f}',\n",
        "                          'acc': f'{accum_acc.avg:.4f}',\n",
        "                          'lr': f'{lr:.6f}',\n",
        "                          'grad_norm': f'{norm:.4f}'})\n",
        "\n",
        "    model.eval()  # evaluation mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    for x, y in test_loader:\n",
        "        with torch.no_grad():\n",
        "            x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "            mask = y != vocab['<PAD>']\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = loss_fn(y_pred, y, mask)\n",
        "            N = mask.int().sum()\n",
        "            accum_loss.update(loss.detach(), N)\n",
        "            accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "    print(f'Epoch{epoch}: val_loss {accum_loss.avg:.4f} val_acc {accum_acc.avg:.4f}')"
      ],
      "metadata": {
        "id": "t6-bXXlDlzgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq2seq val acc = 0.3929, bleu 0.004"
      ],
      "metadata": {
        "id": "464M1BGclzgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode_att(model, x, max_len=128, pad_idx=vocab['<PAD>'], eos_idx=vocab['<|endoftext|>']):\n",
        "    model.eval()\n",
        "    batch_size = x.size(0)\n",
        "    device = x.device\n",
        "\n",
        "    #get the hiddens states of the input sequences\n",
        "    mask_x = x != pad_idx\n",
        "    x = model.embedding(x)\n",
        "    x, _ = model.encoder_rnn(x, None, mask=mask_x)\n",
        "    x = model.source_proj(x)\n",
        "\n",
        "    # Initialize y with <eos> tokens (start of decoding)\n",
        "    pred = torch.full((batch_size, 1), eos_idx, dtype=torch.long, device=device)\n",
        "    curr_token = pred\n",
        "\n",
        "    h = None\n",
        "    # Iteratively decode until max_len or eos token is generated\n",
        "    for i in range(max_len):\n",
        "        # Forward pass through embedding and RNN\n",
        "        y = model.embedding(curr_token)\n",
        "        _, h = model.decoder_rnn(y, h)\n",
        "        y = model.target_proj(h[-1]).unsqueeze(1) #B,1,D\n",
        "\n",
        "        #dot-product attention\n",
        "        att = y @ x.transpose(-2,-1) #B,1,L_x\n",
        "        c = F.softmax(att, dim=-1) @ x #B,1,D\n",
        "\n",
        "        out = model.out_proj(torch.cat([c,y],dim=-1))\n",
        "        logits = model.head(out)[:,0]\n",
        "\n",
        "        # Get logits and select the most probable token (greedy approach)\n",
        "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        # next_token = pad, if seqeunce have generated an <eos> token\n",
        "        if i > 0:\n",
        "            next_token = torch.where((curr_token == eos_idx), eos_idx, next_token)\n",
        "\n",
        "        # Append the predicted token to the sequence\n",
        "        pred = torch.cat([pred, next_token], dim=1)\n",
        "\n",
        "        # Stop if all sequences in the batch have generated an <eos> token\n",
        "        if (next_token == eos_idx).all():\n",
        "            break\n",
        "\n",
        "        curr_token = next_token\n",
        "\n",
        "    # Remove the initial <eos> token from the output\n",
        "    return pred[:, 1:]"
      ],
      "metadata": {
        "id": "Bx85iev9lzgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = greedy_decode_att(model, torch.tensor([tokenizer.encode('I love programming')]).cuda())\n",
        "pred"
      ],
      "metadata": {
        "id": "aFB5efVPlzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(pred[0])\n",
        "#answer: J'adore la programmation"
      ],
      "metadata": {
        "id": "YB80MCRPlzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu"
      ],
      "metadata": {
        "id": "fQgHlVP9lzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU score, 예측문장과 정답 문장이 겹치는 정도를 계산\n",
        "bleu.sentence_bleu([tokenizer.decode(pred[0], skip_special_tokens=True).split()], \"J'adore la programmation\".split())"
      ],
      "metadata": {
        "id": "LAhdbLWslzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "2WJfAtfllzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "preds = []\n",
        "max_len = 128\n",
        "eos_idx = tokenizer.get_vocab()['<|endoftext|>']\n",
        "pad_idx = tokenizer.get_vocab()['<PAD>']\n",
        "for x, y in tqdm(eval_loader):\n",
        "    with torch.no_grad():\n",
        "        x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "        pred = greedy_decode_att(model, x[0], max_len=max_len, pad_idx=pad_idx, eos_idx=eos_idx)\n",
        "        labels.extend(y)\n",
        "        preds.extend(pred)\n",
        "\n",
        "preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "preds = [x.split() for x in preds]\n",
        "labels = [[x.split()] for x in labels]\n",
        "print('EXAMPLE PRED:', ' '.join(preds[0]))\n",
        "print()\n",
        "print('EXAMPLE LABEL:', ' '.join(labels[0][0]))\n",
        "print()\n",
        "print(bleu.corpus_bleu(labels, preds)) #0~1, bigger is better\n"
      ],
      "metadata": {
        "id": "6NZswj-wlzgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ch_FU0UClzgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKsqfl2Slzgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "Xs5XUX_1lzgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSoftmax(nn.Module):\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        # self.softmax = nn.Softmax(self.dim)\n",
        "\n",
        "    def forward(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            inputs = inputs.masked_fill(~mask, torch.finfo(inputs.dtype).min)\n",
        "        return F.softmax(inputs, dim=self.dim)"
      ],
      "metadata": {
        "id": "6bJRbgiVlzgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.dim // self.num_heads\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=True)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=True)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=True)\n",
        "        self.o_proj = nn.Linear(dim, dim, bias=True)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, causal_mask=False):\n",
        "        bs, q_len = q.shape[0], q.shape[1]\n",
        "        k_len = k.shape[1]\n",
        "        q = self.q_proj(q)\n",
        "        k = self.k_proj(k)\n",
        "        v = self.v_proj(v)\n",
        "\n",
        "        q = q.view(bs, q_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = k.view(bs, k_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = v.view(bs, k_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        #padding mask(applied to key=column of attention matrix)\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "        #causal mask for decoder self attention\n",
        "        if causal_mask:\n",
        "            causal_mask = ~torch.triu(torch.ones(q_len, k_len, device=q.device), diagonal=1)[None, None, :, :].bool()\n",
        "            mask = mask & causal_mask if mask is not None else causal_mask\n",
        "\n",
        "        scale = self.head_dim ** -0.5\n",
        "        attn = q @ k.transpose(-2,-1) * scale\n",
        "        attn = MaskedSoftmax(dim=-1)(attn, mask=mask)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = x.permute(0, 2, 1, 3).reshape(bs, q_len, self.dim)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim=256, dropout=0.1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, 4*dim)\n",
        "        self.fc2 = nn.Linear(4*dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU() #nn.GELU, nn.SiLU\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lcJ5YaNolzgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cuasal_mask example\n",
        "~torch.triu(torch.ones(5, 5), diagonal=1).bool()"
      ],
      "metadata": {
        "id": "DTa8GcHNlzg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(dim=dim,num_heads=num_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.ffn = FeedForward(dim, dropout=dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        att_in = x\n",
        "        # x = self.norm1(x) #pre-norm\n",
        "        x = self.attn(q=x,k=x,v=x,mask=mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = x + att_in\n",
        "        x = self.norm1(x) #post-norm\n",
        "\n",
        "        ffn_in = x\n",
        "        # x = self.norm2(x) #pre-norm\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x + ffn_in\n",
        "        x = self.norm2(x) #post-norm\n",
        "        return x\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.self_attn = MultiHeadAttention(dim=dim,num_heads=num_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.cross_attn = MultiHeadAttention(dim=dim,num_heads=num_heads)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.ffn = FeedForward(dim, dropout=dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y, mask_x=None, mask_y=None):\n",
        "        self_att_in = y\n",
        "        # y = self.norm1(y) #pre-norm\n",
        "        y = self.self_attn(q=y,k=y,v=y,mask=mask_y,causal_mask=True)\n",
        "        y = self.dropout1(y)\n",
        "        y = y + self_att_in\n",
        "        y = self.norm1(y) #post-norm\n",
        "\n",
        "        cross_att_in = y\n",
        "        # y = self.norm1(y) #pre-norm\n",
        "        y = self.cross_attn(q=y,k=x,v=x,mask=mask_x)\n",
        "        y = self.dropout2(y)\n",
        "        y = y + cross_att_in\n",
        "        y = self.norm2(y) #post-norm\n",
        "\n",
        "        ffn_in = y\n",
        "        # y = self.norm2(y) #pre-norm\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = y + ffn_in\n",
        "        y = self.norm3(y) #post-norm\n",
        "        return y"
      ],
      "metadata": {
        "id": "iCQCHAfHlzg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=256):\n",
        "        \"\"\"\n",
        "        d_model: Dimension of the model (hidden size)\n",
        "        max_len: Maximum length of the input sequences\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a matrix to hold positional encodings of shape (max_len, d_model)\n",
        "        pos_encoding = torch.zeros((max_len, dim))\n",
        "\n",
        "        # Compute the positional encodings using the formula\n",
        "        position = torch.arange(max_len)[:, None]  # Shape (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / dim))  # Shape (d_model/2)\n",
        "\n",
        "        # Apply sine to even indices in the embedding dimension\n",
        "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cosine to odd indices in the embedding dimension\n",
        "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # add a batch dimension\n",
        "        pos_encoding = pos_encoding.unsqueeze(0)  # Shape (1, max_len, d_model)\n",
        "\n",
        "        # Register the positional encoding as a buffer so it's not trained\n",
        "        # 모델 파라미터는 아니지만 GPU 메모리에 미리 저장되는, requires_grad=False인 tensor\n",
        "        self.register_buffer('pos_encoding', pos_encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "        The input tensor with positional encodings added, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Add positional encoding to the input embeddings\n",
        "        return self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate position embeddings on the fly (extrapolation is possible)\n",
        "        position = torch.arange(x.size(1))[:, None].to(x.device) #(seq_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, self.dim, 2, device=x.device).float() * -(math.log(10000.0) / self.dim))[None, :]  # (1, dim//2)\n",
        "\n",
        "        pe = torch.stack([torch.sin(position * div_term), torch.cos(position * div_term)], dim = -1) #(seq_len, dim/2, 2)\n",
        "        pe = pe.flatten(1) #(seq_len, dim)\n",
        "        return pe"
      ],
      "metadata": {
        "id": "O-v4XPBilzg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, num_layers=2, dropout=0.1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([TransformerEncoderBlock(dim=dim, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.blocks:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, num_layers=2, dropout=0.1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([TransformerDecoderBlock(dim=dim, num_heads=num_heads, dropout=dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, mask_x=None, mask_y=None):\n",
        "        for layer in self.blocks:\n",
        "            y = layer(x, y, mask_x=mask_x, mask_y=mask_y)\n",
        "        return y\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim=256, num_heads=4, num_encoder_layers=2, num_decoder_layers=2, dropout=0.1,\n",
        "                 vocab_size=len(vocab), pad_idx=vocab['<PAD>'], **kwargs):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim)\n",
        "        self.pos_encoding = SinusoidalPositionalEncoding(dim)\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        self.encoder = TransformerEncoder(dim=dim, num_heads=num_heads, num_layers=num_encoder_layers, dropout=dropout)\n",
        "        self.decoder = TransformerDecoder(dim=dim, num_heads=num_heads, num_layers=num_decoder_layers, dropout=dropout)\n",
        "        self.head = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x, y = inp\n",
        "        mask_x = x != self.pad_idx\n",
        "        x = self.embedding(x)\n",
        "        x = x + self.pos_encoding(x)\n",
        "        x = self.emb_dropout(x)\n",
        "        x = self.encoder(x, mask=mask_x)\n",
        "\n",
        "        mask_y = y != self.pad_idx\n",
        "        y = self.embedding(y)\n",
        "        y = y + self.pos_encoding(y)\n",
        "        y = self.emb_dropout(y)\n",
        "        y = self.decoder(x, y, mask_x=mask_x, mask_y=mask_y)\n",
        "        y = self.head(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "M4-PggGLlzhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_loader:\n",
        "    break\n",
        "model = Transformer()\n",
        "model(x[0]).shape"
      ],
      "metadata": {
        "id": "F49fbF8clzhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model)"
      ],
      "metadata": {
        "id": "ADP0U6IRlzhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=42)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = Transformer()\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 5\n",
        "clip_grad = 1.0\n",
        "lr = 1e-3\n",
        "\n",
        "loss_fn = MaskedCCELoss()#CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': [param for param in model.parameters() if param.ndim>=2], 'weight_decay': 0.01},\n",
        "    {'params': [param for param in model.parameters() if param.ndim<2], 'weight_decay': 0.0}\n",
        "], lr=lr)\n",
        "accum_loss = AverageMeter()\n",
        "accum_acc = AverageMeter()\n",
        "total_steps = len(train_loader) * epochs\n",
        "lr_fn = get_cosine_decay_with_warmup(total_steps=total_steps, warmup_steps=total_steps//10, max_lr=lr, min_lr=1e-7)\n",
        "scheduler = LRScheduler(optimizer, lr_fn)\n",
        "best_val_loss = float('inf') #initialize the best valiation loss as infinity\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()  # training mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    pbar = tqdm(train_loader, desc=f'TRAIN epoch {epoch}', total=len(train_loader))\n",
        "    for x, y in pbar:\n",
        "        x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "        mask = y != vocab['<PAD>']\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y, mask)\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        lr = scheduler.step()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        N = mask.int().sum()\n",
        "        accum_loss.update(loss.detach(), N)\n",
        "        accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{accum_loss.avg:.4f}',\n",
        "                          'acc': f'{accum_acc.avg:.4f}',\n",
        "                          'lr': f'{lr:.6f}',\n",
        "                          'grad_norm': f'{norm:.4f}'})\n",
        "\n",
        "    model.eval()  # evaluation mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    for x, y in test_loader:\n",
        "        with torch.no_grad():\n",
        "            x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "            mask = y != vocab['<PAD>']\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = loss_fn(y_pred, y, mask)\n",
        "            N = mask.int().sum()\n",
        "            accum_loss.update(loss.detach(), N)\n",
        "            accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "    print(f'Epoch{epoch}: val_loss {accum_loss.avg:.4f} val_acc {accum_acc.avg:.4f}')"
      ],
      "metadata": {
        "id": "cbDcC8iIlzhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, x, max_len=128, pad_idx=vocab['<PAD>'], eos_idx=vocab['<|endoftext|>']):\n",
        "    model.eval()\n",
        "    batch_size = x.size(0)\n",
        "    device = x.device\n",
        "\n",
        "    #get the hiddens states of the input sequences\n",
        "    mask_x = x != pad_idx\n",
        "    x = model.embedding(x)\n",
        "    x = model.pos_encoding(x) + x\n",
        "    x = model.encoder(x, mask=mask_x)\n",
        "\n",
        "    # Initialize y with <eos> tokens (start of decoding)\n",
        "    y_pred = torch.full((batch_size, 1), eos_idx, dtype=torch.long, device=device)\n",
        "    curr_token = y_pred\n",
        "\n",
        "    # Iteratively decode until max_len or eos token is generated\n",
        "    for i in range(max_len):\n",
        "        # Forward pass through embedding and RNN\n",
        "        y = model.embedding(y_pred)\n",
        "        y = model.pos_encoding(y) + y\n",
        "        y = model.decoder(x, y, mask_x=mask_x)\n",
        "        logits = model.head(y)[:,-1]\n",
        "\n",
        "        # Get logits and select the most probable token (greedy approach)\n",
        "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        # next_token = pad, if seqeunce have generated an <eos> token\n",
        "        if i > 0:\n",
        "            next_token = torch.where((curr_token == eos_idx), eos_idx, next_token)\n",
        "\n",
        "        # Append the predicted token to the sequence\n",
        "        y_pred = torch.cat([y_pred, next_token], dim=1)\n",
        "\n",
        "        # Stop if all sequences in the batch have generated an <eos> token\n",
        "        if (next_token == eos_idx).all():\n",
        "            break\n",
        "\n",
        "        curr_token = next_token\n",
        "\n",
        "    # Remove the initial <eos> token from the output\n",
        "    return y_pred[:, 1:]"
      ],
      "metadata": {
        "id": "MiTMdv6QlzhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = greedy_decode(model, torch.tensor([tokenizer.encode('I love programming')]).cuda())\n",
        "pred"
      ],
      "metadata": {
        "id": "SO21uy5qlzhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(pred[0])"
      ],
      "metadata": {
        "id": "bVDdMDIqlzhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu.sentence_bleu([tokenizer.decode(pred[0], skip_special_tokens=True).split()], \"J'adore la programmation\".split())"
      ],
      "metadata": {
        "id": "CsG5BsO3lzhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "fwL_TUvGlzhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "preds = []\n",
        "max_len = 128\n",
        "eos_idx = tokenizer.get_vocab()['<|endoftext|>']\n",
        "pad_idx = tokenizer.get_vocab()['<PAD>']\n",
        "for x, y in tqdm(eval_loader):\n",
        "    with torch.no_grad():\n",
        "        x, y = (x[0].to(device), x[1].to(device)), y.to(device)\n",
        "        pred = greedy_decode(model, x[0], max_len=max_len, pad_idx=pad_idx, eos_idx=eos_idx)\n",
        "        labels.extend(y)\n",
        "        preds.extend(pred)\n",
        "\n",
        "preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "preds = [x.split() for x in preds]\n",
        "labels = [[x.split()] for x in labels]\n",
        "print('EXAMPLE PRED:', ' '.join(preds[0]))\n",
        "print()\n",
        "print('EXAMPLE LABEL:', ' '.join(labels[0][0]))\n",
        "print()\n",
        "print(bleu.corpus_bleu(labels, preds)) #0~1, bigger is better\n"
      ],
      "metadata": {
        "id": "jr3odE8ZlzhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1nNXC4klzhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT2"
      ],
      "metadata": {
        "id": "P6J1GS1jlzhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "sYecdDC9lzhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2"
      ],
      "metadata": {
        "id": "LLsPmXN3lzhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPT2에는 padding token이 없으므로, embedding에 padding token을 위한 weight vector를 추가해야함.\n",
        "print(gpt2.transformer.wte, len(vocab))"
      ],
      "metadata": {
        "id": "xm3wy0SElzhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "C1npyNMslzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2(torch.tensor([1,2,3,4,5])).logits.shape"
      ],
      "metadata": {
        "id": "B8FqOq73lzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(gpt2)"
      ],
      "metadata": {
        "id": "SWjv_yYKlzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO gpt2의 pretrained embedding만을 가져와 seq2seqattention, transformer학습\n",
        "#hint model.embedding.weight = gpt2.transformer.wte.weight"
      ],
      "metadata": {
        "id": "MbQa_l-3lzhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot Translation with GPT2"
      ],
      "metadata": {
        "id": "7Bp-YifRlzhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "prompt = \"Translate the following text from English into French. English: First Nations Governance was launched, to consult with First Nations peoples on the issues of governance under the Indian Act. French: \"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "gen_tokens = gpt2.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, #sampling-based\n",
        "    max_length=100,\n",
        "    use_cache=True, #KV-Caching\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "print(gen_text)\n",
        "#잘 안되는 이유: 가장 작은 gpt2-small 모델임 + gpt2는 pretraininig data의 90%이상이 영어."
      ],
      "metadata": {
        "id": "uZ7h6VczlzhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning GPT2"
      ],
      "metadata": {
        "id": "DQCvxonolzhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 클래스 정의\n",
        "class WMTSFTDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, train=True):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.df.iloc[idx]\n",
        "        x = data['en']\n",
        "        y = data['fr']\n",
        "        # ' ###'을 delimeter token으로 사용, target과 source sentence를 구분하는 용도\n",
        "        x = f'Translate the following text from English into French.\\nEnglish: {x} ###\\nFrench: '\n",
        "        if self.train:\n",
        "            x += y\n",
        "            x = self.tokenizer.encode(x, return_tensors='pt')[0]\n",
        "            return x\n",
        "        else:\n",
        "            x, y = self.tokenizer.encode(x, return_tensors='pt')[0], self.tokenizer.encode(y, return_tensors='pt')[0]\n",
        "            return x, y"
      ],
      "metadata": {
        "id": "8tAxFsEflzhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(' ###\\nFrench:')"
      ],
      "metadata": {
        "id": "rmpoMZoklzhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(' ###\\nFrench:')"
      ],
      "metadata": {
        "id": "PtPuf-lblzhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_x(batch, max_len=256, pad_idx=vocab['<PAD>'], eos_idx=vocab['<|endoftext|>'], padding_side='right'):\n",
        "    #batch = [(x1,y1),(x2,y2)...]\n",
        "    y = batch #(x1, x2, ...), (y1,y2,...)\n",
        "    y = [s[:max_len] for s in y]\n",
        "    y_lens = [len(s) for s in y]\n",
        "    max_leny = max(y_lens)\n",
        "\n",
        "    #add eos token\n",
        "    y = [torch.cat([s, torch.tensor([eos_idx])]) for s in y]\n",
        "    #left-padding\n",
        "    tar = torch.stack([F.pad(s, (0,max_leny+1-len(s))if padding_side == 'right' else (max_leny+1-len(s),0), value=pad_idx) for s in y])\n",
        "    #shifted-right input for teacher forcing\n",
        "    inp = torch.stack([F.pad(torch.roll(s,1), (0,max_leny+1-len(s))if padding_side == 'right' else (max_leny+1-len(s),0), value=pad_idx) for s in y])\n",
        "\n",
        "    return inp, tar"
      ],
      "metadata": {
        "id": "empmqxoxlzhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = WMTSFTDataset(df[df.split=='train'], tokenizer)\n",
        "test_dataset = WMTSFTDataset(df[df.split=='test'], tokenizer)\n",
        "for x in train_dataset:\n",
        "    print(x)\n",
        "    break"
      ],
      "metadata": {
        "id": "rAWoSnYLlzhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding_side == 'left'\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True, num_workers=4, pin_memory=True, collate_fn=lambda x:collate_fn_x(x,padding_side='left'))\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True, num_workers=4, pin_memory=True, collate_fn=lambda x:collate_fn_x(x,padding_side='left'))\n",
        "for x in train_loader:\n",
        "    print(x[0][0], x[1][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "x1ib_1cVlzhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(seed=42)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.pad_token_id = vocab['<PAD>']\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 1\n",
        "clip_grad = 1.0\n",
        "lr = 2e-4\n",
        "\n",
        "loss_fn = MaskedCCELoss()#CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': [param for param in model.parameters() if param.ndim>=2], 'weight_decay': 0.01},\n",
        "    {'params': [param for param in model.parameters() if param.ndim<2], 'weight_decay': 0.0}\n",
        "], lr=lr)\n",
        "accum_loss = AverageMeter()\n",
        "accum_acc = AverageMeter()\n",
        "total_steps = len(train_loader) * epochs\n",
        "lr_fn = get_cosine_decay_with_warmup(total_steps=total_steps, warmup_steps=total_steps//10, max_lr=lr, min_lr=1e-7)\n",
        "scheduler = LRScheduler(optimizer, lr_fn)\n",
        "best_val_loss = float('inf') #initialize the best valiation loss as infinity\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()  # training mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    pbar = tqdm(train_loader, desc=f'TRAIN epoch {epoch}', total=len(train_loader))\n",
        "    for x, y in pbar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        mask1 = y != vocab['<PAD>']\n",
        "        # delim token 뒤의 모든 y_true값을 True로, 앞의 모든 값을 False으로\n",
        "        mask2 = torch.cumsum(y == vocab['Ġ###'], dim=-1).bool()\n",
        "        mask = mask1 & mask2\n",
        "        y_pred = model(x, attention_mask=(x!=vocab['<PAD>']).float()).logits\n",
        "        loss = loss_fn(y_pred, y, mask)\n",
        "        loss.backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "        lr = scheduler.step()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        N = mask.int().sum()\n",
        "        accum_loss.update(loss.detach(), N)\n",
        "        accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{accum_loss.avg:.4f}',\n",
        "                          'acc': f'{accum_acc.avg:.4f}',\n",
        "                          'lr': f'{lr:.6f}',\n",
        "                          'grad_norm': f'{norm:.4f}'})\n",
        "\n",
        "    model.eval()  # evaluation mode\n",
        "    accum_loss.reset()\n",
        "    accum_acc.reset()\n",
        "    for x, y in test_loader:\n",
        "        with torch.no_grad():\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            mask1 = y != vocab['<PAD>']\n",
        "            # delim token 뒤의 모든 y_true값을 True로, 앞의 모든 값을 False으로\n",
        "            mask2 = torch.cumsum(y == vocab['Ġ###'], dim=-1).bool()\n",
        "            mask = mask1 & mask2\n",
        "            y_pred = model(x, attention_mask=(x!=vocab['<PAD>']).float()).logits\n",
        "\n",
        "            loss = loss_fn(y_pred, y, mask)\n",
        "            N = mask.int().sum()\n",
        "            accum_loss.update(loss.detach(), N)\n",
        "            accum_acc.update(masked_accuracy(y_pred.detach().argmax(-1), y, mask), N)\n",
        "    print(f'Epoch{epoch}: val_loss {accum_loss.avg:.4f} val_acc {accum_acc.avg:.4f}')"
      ],
      "metadata": {
        "id": "TooERG13lzhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(model.generate(inputs=tokenizer.encode(\"Translate the following text from English into French. English: I love programming. French: \", return_tensors='pt').cuda(), max_new_tokens=128)[0])"
      ],
      "metadata": {
        "id": "sWNE0NMLlzhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_eval = WMTSFTDataset(df[df.split=='test'], tokenizer, train=False)\n",
        "test_loader_eval = DataLoader(test_dataset_eval, batch_size=64, shuffle=False, drop_last=False, num_workers=4, pin_memory=True, collate_fn=lambda x:collate_fn(x,teacher_forcing=False,padding_side='left'))"
      ],
      "metadata": {
        "id": "Wk9v8uQklzhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in test_dataset_eval:\n",
        "    print(x)\n",
        "    break"
      ],
      "metadata": {
        "id": "uovwpHjOlzhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "preds = []\n",
        "max_len = 256\n",
        "eos_idx = tokenizer.get_vocab()['<|endoftext|>']\n",
        "pad_idx = tokenizer.get_vocab()['<PAD>']\n",
        "for x, y in tqdm(test_loader_eval):\n",
        "    with torch.no_grad():\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model.generate(inputs=x, max_new_tokens=max_len, use_cache=True, pad_token_id=tokenizer.pad_token_id, attention_mask=(x!=pad_idx).float())\n",
        "        labels.extend(y)\n",
        "        preds.extend(pred)\n",
        "\n",
        "preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "preds = [x[x.find('###\\nFrench: ') + len('###\\nFrench: '):].split() for x in preds]\n",
        "labels = [[x.split()] for x in labels]\n",
        "print('EXAMPLE PRED:', ' '.join(preds[0]))\n",
        "print()\n",
        "print('EXAMPLE LABEL:', ' '.join(labels[0][0]))\n",
        "print()\n",
        "print(bleu.corpus_bleu(labels, preds)) #0~1, bigger is better"
      ],
      "metadata": {
        "id": "7P6UdH57lzhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4jeQo_AlzhP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}