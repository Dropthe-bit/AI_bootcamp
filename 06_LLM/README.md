# π“ 06_LLM: Transformer & GPT2 κΈ°λ° λ²μ—­ λ° νμΈνλ‹ μ‹¤μµ

μμ—°μ–΄μ²λ¦¬ λ¨λΈμ κµ¬μ΅°μΈ Seq2Seq, Transformer, GPT2μ— λ€ν• μ‹¤μµ μμ κ°€ λ‹΄κ²¨ μμµλ‹λ‹¤.  
Attention λ©”μ»¤λ‹μ¦, ν¬μ§€μ…”λ„ μΈμ½”λ”©, λ©€ν‹°ν—¤λ“ μ–΄ν…μ… κµ¬ν„λ¶€ν„° GPT2 κΈ°λ° μ λ΅μƒ· λ²μ—­, νμΈνλ‹κΉμ§€ ν•™μµ κ³Όμ • μ‹¤μµ.

## π“„ μ‹¤μµ λ©λ΅

### β [Seq2Seq + Attention Mechanism, Transformer, GPT2 μ‹¤μµ ν†µν•©](https://colab.research.google.com/github/Dropthe-bit/ai_portfolio/blob/main/06_LLM/6_transfomer.ipynb)
- λ²μ—­μ„ μ„ν• GRU κΈ°λ° Seq2Seq λ¨λΈ κµ¬ν„
- Attention λ©”μ»¤λ‹μ¦ μ μ©ν•μ—¬ λ¬Έλ§¥ μ •λ³΄ κ°•ν™”
- Transformerμ ν¬μ§€μ…”λ„ μΈμ½”λ”©, λ©€ν‹°ν—¤λ“ μ–΄ν…μ… λ“± ν•µμ‹¬ λ¨λ“ μ§μ ‘ κµ¬ν„
- Hugging Faceμ GPT2Tokenizer μ‹¤μµ λ° μ λ΅μƒ· λ²μ—­
- μ‚¬μ „ν•™μµλ GPT2λ¥Ό λ²μ—­ νƒμ¤ν¬μ— λ§κ² νμΈνλ‹

<details>
<summary>π“ ν¬ν•¨λ μ‹¤μµ ν•­λ© λ³΄κΈ°</summary>

- GPT2Tokenizerλ¥Ό ν™μ©ν• ν† ν¬λ‚μ΄μ§•  
- `<PAD>` λ° `<|endoftext|>` ν† ν° μ²λ¦¬  
- μ»¤μ¤ν…€ Dataset (`WMTDataset`) λ° `collate_fn` κµ¬μ„±  
- Seq2Seq λ¨λΈ ν΄λμ¤ λ° Attention λ¨λ“ κµ¬ν„  
- ν¬μ§€μ…”λ„ μΈμ½”λ”©, λ©€ν‹°ν—¤λ“ μ–΄ν…μ…, Feedforward, LayerNorm μ§μ ‘ κµ¬ν„  
- Transformer μΈμ½”λ”-λ””μ½”λ” κµ¬μ΅° μ‘μ„±  
- Cosine decay warmup ν•™μµλ¥  μ¤μΌ€μ¤„λ¬ κµ¬ν„  
- GPT2 κΈ°λ° μ λ΅μƒ· λ²μ—­ μ‹¤μµ 
- GPT2 νμΈνλ‹μ„ μ„ν• μ»¤μ¤ν…€ ν•™μµ λ£¨ν”„
</details>
